Learning Objective
Explain the core concepts of LLMs (tokens, context) and their limitations (hallucinations, bias).

Terms from videos
1. Parameters
2. Back-propagation
3. Re-enforced learning
4. Transformer
5. Attention

What are Large Language Models?
Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. 
The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. 
The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.

Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning

GPT: Generative Pre-trained Transformer
Embedding: An embedding is a vector representation of a piece of data (e.g. some text). They are long lists of numbers (vectors) that represent the meaning of a token.
that is meant to preserve aspects of its content and/or its meaning.
Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. 
Tokens -> embeddings



